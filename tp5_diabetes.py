# -*- coding: utf-8 -*-
"""TP5_DIABETES.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZhOCjAWuuwUuFzqtGsyT-ERQhl8npwQc
"""

import pandas as pd
from sklearn import tree
import numpy as np 
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV

data = pd.read_csv('diabetes.csv')

data.head()

X = data.iloc[:,[0,2]].values


X.shape

print(X)

y = data.iloc[:,-1].values


y.shape

print(y)

print(X.shape,y.shape)

from sklearn.model_selection import train_test_split 
X_train,  X_test,  y_train,  y_test  =  train_test_split(X,  y,  train_size=0.7, random_state=0)

from sklearn import svm, datasets 
C = 1.0 # paramètre de régularisation 
lin_svc = svm.LinearSVC(C=C) 
lin_svc.fit(X_train, y_train)

lin_svc.score(X_train, y_train)

#Réponse : le score d’échantillons bien classifiés sur le jeu de données de test est 
0.64



from sklearn.svm import SVC
C = [1,30,60,90,110]
for C in C :
    lin_svc = svm.LinearSVC(C=C)
    lin_svc.fit(X_train, y_train)
    lin_svc.score(X_train, y_train)

    plt.figure(C)
    print('valeur de C :', C)
    print('le score est : ',lin_svc.score(X_test, y_test))
     # Créer la surface de décision discretisée 
    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1 
    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1 
    # Pour afficher la surface de décision on va discrétiser l'espace avec un 
    h = max((x_max - x_min) / 100, (y_max - y_min) / 100) 
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) 

    # Surface de décision 
    Z = lin_svc.predict(np.c_[xx.ravel(), yy.ravel()]) 
    Z = Z.reshape(xx.shape) 

    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8) 
    # Afficher aussi les points d'apprentissage 
    plt.scatter(X_train[:,  0],  X_train[:,  1],  label="train",  edgecolors='k', 
    c=y_train, cmap=plt.cm.coolwarm) 
    plt.scatter(X_test[:, 0], X_test[:, 1], label="test", marker='*', c=y_test, 
    cmap=plt.cm.coolwarm) 
    plt.xlabel('Sepal length') 
    plt.ylabel('Sepal width') 
    plt.title("LinearSVC")

from sklearn.preprocessing import StandardScaler
X = data.iloc[:,[0,4]].values
print(np.unique(X))
#X,y = data.iloc[:,:2] , .target
scaler = StandardScaler()
scaler.fit(X)
X = scaler.transform(X)
print(np.unique(X))
X_train,  X_test,  y_train,  y_test  =  train_test_split(X,  y,  train_size=0.7, random_state=0)

print('le score est : ',lin_svc.score(X_test, y_test))

from sklearn.model_selection import GridSearchCV

parameters = { 'kernel':['linear', 'rbf', 'poly'], 'C':[1, 30,60,90,110], 'gamma' :[0.5] }


grid = GridSearchCV(SVC(), parameters, cv = 5 )
grid.fit(X_train,y_train)


lin_svc = svm.LinearSVC(C=C, loss = 'hinge', random_state=0)
lin_svc.fit(X_train, y_train)
lin_svc.score(X_train, y_train)


# Afficher aussi les points d'apprentissage 
plt.scatter(X_train[:,  0],  X_train[:,  1],  label="train",  edgecolors='k', c=y_train, cmap=plt.cm.coolwarm) 
plt.scatter(X_test[:, 0], X_test[:, 1], label="test", marker='*', c=y_test, 
cmap=plt.cm.coolwarm) 
plt.xlabel('Sepal length') 
plt.ylabel('Sepal width') 
plt.title("LinearSVC")

grid.best_params_

grid.score(X_train, y_train)



from sklearn.model_selection import GridSearchCV

parameters = { 'kernel':['rbf' ], 'C':[1, 30,60,90,110], 'gamma' :[0.5] }


grid = GridSearchCV(SVC(), parameters, cv = 5 )
grid.fit(X_train,y_train)


lin_svc = svm.LinearSVC(C=C, loss = 'hinge', random_state=0)
lin_svc.fit(X_train, y_train)
lin_svc.score(X_train, y_train)


# Afficher aussi les points d'apprentissage 
plt.figure(2)
plt.scatter(X_train[:,  0],  X_train[:,  1],  label="train",  edgecolors='k', c=y_train, cmap=plt.cm.coolwarm) 
plt.scatter(X_test[:, 0], X_test[:, 1], label="test", marker='*', c=y_test, 
cmap=plt.cm.coolwarm) 
plt.xlabel('Sepal length') 
plt.ylabel('Sepal width') 
plt.title("LinearSVC")



lin_svc = svm.LinearSVC(C=C).fit(X_train, y_train) 
svc = svm.SVC(kernel='linear', C=C).fit(X_train, y_train) 
 
titles = ['SVC with linear kernel', 'LinearSVC (linear kernel)'] 
 
fig = plt.figure(figsize=(12, 5)) 
 
for i, clf in enumerate((svc, lin_svc)): 
    plt.subplot(1, 2, i + 1) 
    plt.subplots_adjust(wspace=0.4, hspace=0.4) 
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) 
    # Utiliser une palette de couleurs 
    Z = Z.reshape(xx.shape) 
    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8) 
    # Afficher aussi les points d'apprentissage 
    plt.scatter(X_train[:, 0], X_train[:, 1], label="train", edgecolors='k', 
    c=y_train, cmap=plt.cm.coolwarm) 
    plt.scatter(X_test[:, 0], X_test[:, 1], label="test", marker='*', 
    c=y_test, cmap=plt.cm.coolwarm) 
    plt.xlabel('Sepal length')
    plt.xlim(-1,4)
    plt.ylabel('Sepal width')
    plt.ylim(-3,3)
    plt.title(titles[i]) 
plt.show()

print(' le score est ', lin_svc.score(X_train, y_train))

